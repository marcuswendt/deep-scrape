#!/bin/bash

# Media Downloader
# Downloads images and videos from a URL, recursively crawling linked pages

set -uo pipefail

# Default configuration
DEPTH=1
OUTPUT_DIR=""
CONCURRENCY=5
VERBOSE=false
DRY_RUN=false
DELAY_MS=100
TIMEOUT=30
MAX_RETRIES=3
MIN_WIDTH=0
MIN_HEIGHT=0
SKIP_DUPLICATES=true
VISUAL_DEDUP=true

# File extensions to download
IMAGE_EXTENSIONS="jpg|jpeg|png|gif|webp|svg|avif|ico|JPG|JPEG|PNG|GIF|WEBP|SVG|AVIF|ICO"
VIDEO_EXTENSIONS="mp4|webm|mov|avi|mkv|MP4|WEBM|MOV|AVI|MKV"
MEDIA_EXTENSIONS="$IMAGE_EXTENSIONS|$VIDEO_EXTENSIONS"

# Tracking files
VISITED_FILE=""
ALLOWED_DOMAINS_FILE=""
DOWNLOADED_FILE=""
HASHES_FILE=""
LOG_FILE=""

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_verbose() { [[ "$VERBOSE" == "true" ]] && echo -e "${BLUE}[DEBUG]${NC} $1" >&2 || true; }
log_success() { echo -e "${GREEN}[OK]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1" >&2; }

usage() {
    cat << EOF
Usage: $(basename "$0") <url> [options]

Downloads images and videos from a URL, recursively crawling linked pages.

Arguments:
    <url>                   Source URL to crawl (required)

Options:
    -d, --depth <n>         Recursion depth (0 = initial page only) [default: 1]
    -o, --output <dir>      Output directory [default: ./<domain>]
    -c, --concurrency <n>   Parallel downloads [default: 5]
    -v, --verbose           Verbose output
    --dry-run               Show what would be downloaded
    --min-width <px>        Minimum image width [default: 0]
    --min-height <px>       Minimum image height [default: 0]
    --min-dim <px>          Set both min-width and min-height
    --allow-duplicates      Allow duplicate files
    --no-visual-dedup       Disable visual similarity detection
    -h, --help              Show this help message

Examples:
    $(basename "$0") https://example.com/
    $(basename "$0") https://example.com -d 2 -o ~/Pictures/scraped
    $(basename "$0") https://example.com --min-dim 800
EOF
    exit 0
}

get_domain() {
    echo "$1" | sed -E 's|^https?://([^/]+).*|\1|' | sed 's|^www\.||'
}

get_base_url() {
    echo "$1" | sed -E 's|^(https?://[^/]+).*|\1|'
}

normalize_url() {
    local url="$1" base_url="$2"
    [[ "$url" =~ ^https?:// ]] && { echo "$url"; return; }
    [[ "$url" =~ ^// ]] && { echo "https:$url"; return; }
    [[ "$url" =~ ^/ ]] && { echo "$(get_base_url "$base_url")$url"; return; }
    echo "$(get_base_url "$base_url")/$url"
}

is_media_url() {
    local url="$1"
    local clean_url="${url%%[?#]*}"
    [[ "$clean_url" =~ \.($MEDIA_EXTENSIONS)$ ]]
}

is_domain_allowed() {
    local domain="${1#www.}"
    grep -qFx "$domain" "$ALLOWED_DOMAINS_FILE" 2>/dev/null
}

is_visited() { grep -qFx "$1" "$VISITED_FILE" 2>/dev/null; }
mark_visited() { echo "$1" >> "$VISITED_FILE"; }
is_downloaded() { grep -qFx "$1" "$DOWNLOADED_FILE" 2>/dev/null; }
mark_downloaded() { echo "$1" >> "$DOWNLOADED_FILE"; }

get_file_hash() { md5 -q "$1" 2>/dev/null; }
is_duplicate_hash() {
    [[ "$SKIP_DUPLICATES" != "true" ]] && return 1
    grep -qFx "$1" "$HASHES_FILE" 2>/dev/null
}
record_hash() { echo "$1" >> "$HASHES_FILE"; }

check_image_dimensions() {
    local filepath="$1"
    [[ $MIN_WIDTH -eq 0 ]] && [[ $MIN_HEIGHT -eq 0 ]] && return 0

    local ext="${filepath##*.}"
    ext=$(echo "$ext" | tr '[:upper:]' '[:lower:]')
    case "$ext" in
        jpg|jpeg|png|gif|webp|avif) ;;
        *) return 0 ;;
    esac

    local width height
    width=$(sips -g pixelWidth "$filepath" 2>/dev/null | tail -1 | awk '{print $2}')
    height=$(sips -g pixelHeight "$filepath" 2>/dev/null | tail -1 | awk '{print $2}')
    [[ -z "$width" ]] || [[ -z "$height" ]] && return 0
    [[ $MIN_WIDTH -gt 0 ]] && [[ $width -lt $MIN_WIDTH ]] && return 1
    [[ $MIN_HEIGHT -gt 0 ]] && [[ $height -lt $MIN_HEIGHT ]] && return 1
    return 0
}

fetch_page() {
    curl -sL --max-time "$TIMEOUT" \
        -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36" \
        "$1" 2>/dev/null
}

extract_media_urls() {
    local base_url="$2"
    local temp_file="$1"  # Already a file path

    {
        grep -oE 'src="[^"]+\.(jpg|jpeg|png|gif|webp|svg|avif|ico|mp4|webm|mov)[^"]*"' "$temp_file" 2>/dev/null | sed 's/src="//;s/"$//'
        grep -oE 'data-src="[^"]+"' "$temp_file" 2>/dev/null | sed 's/data-src="//;s/"$//'
        grep -oE 'data-image="[^"]+"' "$temp_file" 2>/dev/null | sed 's/data-image="//;s/"$//'
        grep -oE 'poster="[^"]+"' "$temp_file" 2>/dev/null | sed 's/poster="//;s/"$//'
        grep -oE 'https://images\.squarespace-cdn\.com/[^"'"'"'<>\\]+\.(jpg|jpeg|png|gif|webp)' "$temp_file" 2>/dev/null || true
    } | while read -r url; do
        [[ -z "$url" ]] && continue
        local normalized
        normalized=$(normalize_url "$url" "$base_url")
        is_media_url "$normalized" && echo "$normalized"
    done | sort -u
}

extract_page_links() {
    local temp_file="$1" base_url="$2" source_domain="$3"
    local link_url  # Use different variable to avoid conflicts

    # Extract hrefs and process them
    grep -oE 'href="[^"]+"' "$temp_file" 2>/dev/null | sed 's/href="//;s/"$//' | while IFS= read -r link_url; do
        [[ -z "$link_url" ]] && continue
        [[ "$link_url" =~ ^(javascript:|mailto:) ]] && continue

        # Handle hash-based URLs (e.g., #/gallery-name/ -> /gallery-name/)
        if [[ "$link_url" =~ ^#/ ]]; then
            link_url="${link_url:1}"  # Remove leading #
        elif [[ "$link_url" =~ ^# ]]; then
            continue  # Skip plain anchor links like #top
        fi

        # Normalize URL - inline to avoid subshell issues
        local normalized
        if [[ "$link_url" =~ ^https?:// ]]; then
            normalized="$link_url"
        elif [[ "$link_url" =~ ^// ]]; then
            normalized="https:$link_url"
        elif [[ "$link_url" =~ ^/ ]]; then
            normalized="$(echo "$base_url" | sed -E 's|^(https?://[^/]+).*|\1|')$link_url"
        else
            normalized="$(echo "$base_url" | sed -E 's|^(https?://[^/]+).*|\1|')/$link_url"
        fi

        # Get domain - inline
        local domain
        domain=$(echo "$normalized" | sed -E 's|^https?://([^/]+).*|\1|' | sed 's|^www\.||')

        if [[ "$domain" == "$source_domain" ]] || [[ "$domain" == "www.$source_domain" ]]; then
            echo "$normalized"
        fi
    done | sort -u
}

extract_domains_from_urls() {
    local temp_file="$1"
    grep -oE 'https?://[^/"'"'"'<> ]+' "$temp_file" 2>/dev/null | \
        sed -E 's|^https?://([^/]+).*|\1|' | sed 's|^www\.||' | sort -u
}

download_file() {
    local url="$1"
    local domain=$(get_domain "$url")
    local domain_dir="$OUTPUT_DIR/$domain"
    mkdir -p "$domain_dir"

    local filename="${url%%[?#]*}"
    filename="${filename##*/}"
    [[ -z "$filename" ]] && filename="file_$(date +%s)"

    local filepath="$domain_dir/$filename"
    local counter=1
    local base="${filename%.*}" ext="${filename##*.}"
    [[ "$base" == "$ext" ]] && ext=""

    while [[ -f "$filepath" ]]; do
        [[ -n "$ext" ]] && filepath="$domain_dir/${base}_${counter}.${ext}" || filepath="$domain_dir/${base}_${counter}"
        counter=$((counter + 1))
    done

    if [[ "$DRY_RUN" == "true" ]]; then
        echo "[DRY-RUN] Would download: $url -> $filepath"
        return 0
    fi

    local retry=0
    while [[ $retry -lt $MAX_RETRIES ]]; do
        if curl -sL --max-time "$TIMEOUT" \
            -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36" \
            -o "$filepath" "$url" 2>/dev/null; then

            if [[ -f "$filepath" ]] && [[ -s "$filepath" ]]; then
                local file_hash=$(get_file_hash "$filepath")
                if is_duplicate_hash "$file_hash"; then
                    log_verbose "Skipping duplicate: $filename"
                    rm -f "$filepath"
                    return 0
                fi
                if ! check_image_dimensions "$filepath"; then
                    log_verbose "Skipping (too small): $filename"
                    rm -f "$filepath"
                    return 0
                fi
                record_hash "$file_hash"
                log_success "Downloaded: $filename"
                return 0
            fi
        fi
        retry=$((retry + 1))
        sleep 1
    done
    log_error "Failed: $url"
    rm -f "$filepath" 2>/dev/null
    return 1
}

process_downloads() {
    local urls_file="$1"
    local pids=()
    local download_url  # Use different name to avoid shadowing caller's $url

    while IFS= read -r download_url || [[ -n "$download_url" ]]; do
        [[ -z "$download_url" ]] && continue
        is_downloaded "$download_url" && continue
        local domain=$(get_domain "$download_url")
        is_domain_allowed "$domain" || continue
        mark_downloaded "$download_url"

        download_file "$download_url" &
        pids+=($!)

        if [[ ${#pids[@]} -ge $CONCURRENCY ]]; then
            wait "${pids[0]}" 2>/dev/null || true
            pids=("${pids[@]:1}")
        fi
        sleep "0.$(printf '%03d' $DELAY_MS)"
    done < "$urls_file"

    if [[ ${#pids[@]} -gt 0 ]]; then
        for pid in "${pids[@]}"; do
            wait "$pid" 2>/dev/null || true
        done
    fi
}

get_visual_hash() {
    local filepath="$1"
    local ext="${filepath##*.}"
    ext=$(echo "$ext" | tr '[:upper:]' '[:lower:]')
    case "$ext" in
        jpg|jpeg|png|gif|webp|avif) ;;
        *) return ;;
    esac

    local temp_dir=$(mktemp -d)
    local temp_thumb="$temp_dir/thumb.tiff"
    if sips -z 8 8 -s format tiff "$filepath" --out "$temp_thumb" &>/dev/null; then
        md5 -q "$temp_thumb" 2>/dev/null
    fi
    rm -rf "$temp_dir"
}

get_image_pixels() {
    local w h
    w=$(sips -g pixelWidth "$1" 2>/dev/null | tail -1 | awk '{print $2}')
    h=$(sips -g pixelHeight "$1" 2>/dev/null | tail -1 | awk '{print $2}')
    [[ -n "$w" ]] && [[ -n "$h" ]] && echo $((w * h)) || echo "0"
}

deduplicate_visual_similar() {
    [[ "$VISUAL_DEDUP" != "true" ]] || [[ "$DRY_RUN" == "true" ]] && return

    log "Scanning for visually similar images..."
    local images_file=$(mktemp)
    find "$OUTPUT_DIR" -type f \( -iname "*.jpg" -o -iname "*.jpeg" -o -iname "*.png" -o -iname "*.gif" -o -iname "*.webp" \) 2>/dev/null > "$images_file"

    local count=$(wc -l < "$images_file" | tr -d ' ')
    [[ $count -lt 2 ]] && { rm -f "$images_file"; return; }

    log_verbose "Analyzing $count images..."
    local hash_map=$(mktemp)

    while IFS= read -r f; do
        [[ -f "$f" ]] || continue
        local vh=$(get_visual_hash "$f")
        [[ -n "$vh" ]] && echo "${vh}|${f}|$(get_image_pixels "$f")|$(stat -f%z "$f" 2>/dev/null)" >> "$hash_map"
    done < "$images_file"

    local removed=0
    for hash in $(cut -d'|' -f1 "$hash_map" | sort | uniq -d); do
        local first=true
        grep "^${hash}|" "$hash_map" | sort -t'|' -k3,3nr -k4,4nr | while IFS='|' read -r h f p s; do
            if $first; then
                first=false
                log_verbose "Keeping: $f"
            else
                [[ -f "$f" ]] && { rm -f "$f"; log_verbose "Removed duplicate: $f"; removed=$((removed + 1)); }
            fi
        done
    done

    rm -f "$images_file" "$hash_map"
    [[ $removed -gt 0 ]] && log "Removed $removed visual duplicates"
}

crawl_page() {
    local url="$1" depth="$2" source_domain="$3"
    [[ $depth -lt 0 ]] && return
    is_visited "$url" && return
    mark_visited "$url"

    log "Crawling (depth=$depth): $url"
    local content=$(fetch_page "$url")
    [[ -z "$content" ]] && { log_warn "Could not fetch: $url"; return; }

    local temp_file=$(mktemp)
    echo "$content" > "$temp_file"

    local media_file=$(mktemp)
    extract_media_urls "$temp_file" "$url" > "$media_file"
    local media_count=$(wc -l < "$media_file" | tr -d ' ')
    log "Found $media_count media files"

    process_downloads "$media_file"
    rm -f "$media_file"

    if [[ $depth -gt 0 ]]; then
        local links_file=$(mktemp)
        extract_page_links "$temp_file" "$url" "$source_domain" > "$links_file"
        local link_count=$(wc -l < "$links_file" | tr -d ' ')
        log_verbose "Found $link_count page links to crawl"
        while IFS= read -r link; do
            if [[ -n "$link" ]]; then
                log_verbose "Processing link: $link"
                crawl_page "$link" $((depth - 1)) "$source_domain"
            fi
        done < "$links_file"
        rm -f "$links_file"
    fi

    rm -f "$temp_file"
}

main() {
    local url=""

    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help) usage ;;
            -d|--depth) DEPTH="$2"; shift 2 ;;
            -o|--output) OUTPUT_DIR="$2"; shift 2 ;;
            -c|--concurrency) CONCURRENCY="$2"; shift 2 ;;
            -v|--verbose) VERBOSE=true; shift ;;
            --dry-run) DRY_RUN=true; shift ;;
            --min-width) MIN_WIDTH="$2"; shift 2 ;;
            --min-height) MIN_HEIGHT="$2"; shift 2 ;;
            --min-dim) MIN_WIDTH="$2"; MIN_HEIGHT="$2"; shift 2 ;;
            --allow-duplicates) SKIP_DUPLICATES=false; shift ;;
            --no-visual-dedup) VISUAL_DEDUP=false; shift ;;
            -*) log_error "Unknown option: $1"; usage ;;
            *) [[ -z "$url" ]] && url="$1" || { log_error "Unexpected: $1"; usage; }; shift ;;
        esac
    done

    [[ -z "$url" ]] && { log_error "URL required"; usage; }
    [[ ! "$url" =~ ^https?:// ]] && url="https://$url"
    [[ -z "$OUTPUT_DIR" ]] && OUTPUT_DIR="./$(get_domain "$url")"

    mkdir -p "$OUTPUT_DIR"
    VISITED_FILE="$OUTPUT_DIR/.visited"
    ALLOWED_DOMAINS_FILE="$OUTPUT_DIR/.allowed_domains"
    DOWNLOADED_FILE="$OUTPUT_DIR/.downloaded"
    HASHES_FILE="$OUTPUT_DIR/.hashes"
    LOG_FILE="$OUTPUT_DIR/download.log"

    > "$VISITED_FILE"; > "$ALLOWED_DOMAINS_FILE"; > "$DOWNLOADED_FILE"; > "$HASHES_FILE"

    local source_domain=$(get_domain "$url")

    log "Starting download from: $url"
    log "Source domain: $source_domain"
    log "Recursion depth: $DEPTH"
    log "Output directory: $OUTPUT_DIR"
    [[ $MIN_WIDTH -gt 0 ]] || [[ $MIN_HEIGHT -gt 0 ]] && log "Minimum dimensions: ${MIN_WIDTH}x${MIN_HEIGHT}px"
    [[ "$SKIP_DUPLICATES" == "true" ]] && log "Duplicate detection: enabled"
    [[ "$VISUAL_DEDUP" == "true" ]] && log "Visual similarity detection: enabled"
    [[ "$DRY_RUN" == "true" ]] && log "DRY RUN MODE"
    echo ""

    log "Discovering allowed domains..."
    local content=$(fetch_page "$url")
    [[ -z "$content" ]] && { log_error "Could not fetch: $url"; exit 1; }

    local temp_file=$(mktemp)
    echo "$content" > "$temp_file"

    echo "$source_domain" >> "$ALLOWED_DOMAINS_FILE"
    extract_domains_from_urls "$temp_file" >> "$ALLOWED_DOMAINS_FILE"
    sort -u "$ALLOWED_DOMAINS_FILE" -o "$ALLOWED_DOMAINS_FILE"

    # Filter to only keep relevant domains (source + CDN domains with media)
    local filtered_domains=$(mktemp)
    echo "$source_domain" > "$filtered_domains"
    grep -E 'squarespace-cdn|cloudinary|imgix|cdn|static|images\.' "$ALLOWED_DOMAINS_FILE" >> "$filtered_domains" 2>/dev/null || true
    sort -u "$filtered_domains" -o "$ALLOWED_DOMAINS_FILE"
    rm -f "$filtered_domains"

    log "Allowed domains:"
    while IFS= read -r d; do log "  - $d"; done < "$ALLOWED_DOMAINS_FILE"
    echo ""

    rm -f "$temp_file"

    crawl_page "$url" "$DEPTH" "$source_domain"

    echo ""
    deduplicate_visual_similar

    echo ""
    log "Download complete!"
    local total=$(find "$OUTPUT_DIR" -type f \( -iname "*.jpg" -o -iname "*.jpeg" -o -iname "*.png" -o -iname "*.gif" -o -iname "*.webp" -o -iname "*.mp4" \) 2>/dev/null | wc -l | tr -d ' ')
    log "Total files: $total"
    log "Saved to: $OUTPUT_DIR"

    rm -f "$VISITED_FILE" "$ALLOWED_DOMAINS_FILE" "$DOWNLOADED_FILE" "$HASHES_FILE"
}

main "$@"
